{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfV4QkvDsRKys7pA9fgZZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh152003/Reinforcement_Learning/blob/main/Exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5nx3ysgsS6h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "class GridWorld:\n",
        "    def __init__(self, rows, cols):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.state_space = rows * cols\n",
        "        self.action_space = 4  # 0: up, 1: down, 2: left, 3: right\n",
        "        self.grid = np.zeros((rows, cols))\n",
        "\n",
        "        # Define rewards and obstacles\n",
        "        self.rewards = {(rows - 1, cols - 1): 1}  # Goal state\n",
        "        self.obstacles = [(1, 1)]  # Obstacle at (1, 1)\n",
        "\n",
        "        # Initialize Q-values\n",
        "        self.Q = np.zeros((self.state_space, self.action_space))\n",
        "\n",
        "    def get_state(self, row, col):\n",
        "        return row * self.cols + col\n",
        "\n",
        "\n",
        "# Define the environment\n",
        "class GridWorld:\n",
        "    def __init__(self, rows, cols):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.state_space = rows * cols\n",
        "        self.action_space = 4  # 0: up, 1: down, 2: left, 3: right\n",
        "        self.grid = np.zeros((rows, cols))\n",
        "\n",
        "        # Define rewards and obstacles\n",
        "        self.rewards = {(rows - 1, cols - 1): 1}  # Goal state\n",
        "        self.obstacles = [(1, 1)]  # Obstacle at (1, 1)\n",
        "\n",
        "        # Initialize Q-values\n",
        "        self.Q = np.zeros((self.state_space, self.action_space))\n",
        "\n",
        "    def get_state(self, row, col):\n",
        "        return row * self.cols + col\n",
        "    def is_valid_state(self, row, col):\n",
        "        return 0 <= row < self.rows and 0 <= col < self.cols and (row, col) not in self.obstacles\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        current_row, current_col = divmod(state, self.cols)\n",
        "\n",
        "        if action == 0:  # Move up\n",
        "            next_row = current_row - 1\n",
        "            next_col = current_col\n",
        "        elif action == 1:  # Move down\n",
        "            next_row = current_row + 1\n",
        "            next_col = current_col\n",
        "        elif action == 2:  # Move left\n",
        "            next_row = current_row\n",
        "            next_col = current_col - 1\n",
        "        elif action == 3:  # Move right\n",
        "            next_row = current_row\n",
        "            next_col = current_col + 1\n",
        "\n",
        "        if self.is_valid_state(next_row, next_col):\n",
        "            return self.get_state(next_row, next_col)\n",
        "        else:\n",
        "            return state\n",
        "\n",
        "# Q-learning algorithm\n",
        "def q_learning(env, num_episodes, learning_rate, discount_factor, exploration_prob):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.get_state(0, 0)\n",
        "\n",
        "        while state not in env.rewards:\n",
        "            # Epsilon-greedy exploration-exploitation strategy\n",
        "            if np.random.rand() < exploration_prob:\n",
        "                action = np.random.choice(env.action_space)\n",
        "            else:\n",
        "                action = np.argmax(env.Q[state])\n",
        "\n",
        "            next_state = env.take_action(state, action)\n",
        "            reward = env.rewards.get(next_state, 0)\n",
        "\n",
        "            # Q-value update\n",
        "            best_next_action_value = np.max(env.Q[next_state])\n",
        "            env.Q[state, action] += learning_rate * (reward + discount_factor *best_next_action_value - env.Q[state, action])\n",
        "            state = next_state\n",
        "\n",
        "# Function to visualize the learned policy\n",
        "def visualize_policy(env):\n",
        "    policy = np.argmax(env.Q, axis=1).reshape((env.rows, env.cols))\n",
        "    print(\"Learned Policy:\")\n",
        "    print(policy)\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    rows, cols = 3, 3\n",
        "    env = GridWorld(rows, cols)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    learning_rate = 0.1\n",
        "    discount_factor = 0.9\n",
        "    exploration_prob = 0.2\n",
        "\n",
        "    q_learning(env, num_episodes, learning_rate, discount_factor, exploration_prob)\n",
        "    visualize_policy(env)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}